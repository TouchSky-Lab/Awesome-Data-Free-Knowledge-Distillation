


# Data-Free Knowledge Distillation
- Data-Free Knowledge Distillation for Deep Neural Networks. NeurIPS 2017
- Zero-Shot Knowledge Distillation in Deep Networks. ICML 2019
- DAFL:Data-Free Learning of Student Networks. ICCV 2019
- Zero-shot Knowledge Transfer via Adversarial Belief Matching. Micaelli, Paul and Storkey, Amos. NeurIPS 2019
- Dream Distillation: A Data-Independent Model Compression Framework. Kartikeya et al. ICML 2019
- Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion. Yin, Hongxu et al. CVPR 2020 [code]
- Data-Free Adversarial Distillation. Fang, Gongfan et al. CVPR 2020
- The Knowledge Within: Methods for Data-Free Model Compression. Haroush, Matan et al. CVPR 2020
- Knowledge Extraction with No Observable Data. Yoo, Jaemin et al. NeurIPS 2019 [code]
- Data-Free Knowledge Amalgamation via Group-Stack Dual-GAN. CVPR 2020
- DeGAN: Data-Enriching GAN for Retrieving Representative Samples from a Trained Classifier. Addepalli, Sravanti et al. arXiv:1912.11960
- This dataset does not exist: training models from generated images. arXiv:1911.02888
- Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data. Such et al. ECCV 2020
- Billion-scale semi-supervised learning for image classification. FAIR. arXiv:1905.00546 [code]
- Adversarial Self-Supervised Data-Free Distillation for Text Classification. EMNLP 2020
- Data-free Knowledge Distillation for Segmentation using Data-Enriching GAN. Bhogale et al. arXiv:2011.00809
- Layer-Wise Data-Free CNN Compression. Horton, Maxwell et al (Apple Inc.). cvpr 2021
- Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge Distillation. Nayak et al. WACV 2021
- Large-Scale Generative Data-Free Distillation. Luo, Liangchen et al. cvpr 2021
- Learning Student Networks in the Wild. (HUAWEI-Noah). CVPR 2021
- Data-Free Knowledge Distillation For Image Super-Resolution. (HUAWEI-Noah). CVPR 2021
- Data-Free Model Extraction. Jean-Baptiste et al. CVPR 2021 [code]
- Zero-Shot Knowledge Distillation Using Label-Free Adversarial Perturbation With Taylor Approximation. Li, Kang et al. IEEE Access, 2021.
- Half-Real Half-Fake Distillation for Class-Incremental Semantic Segmentation. Huang, Zilong et al. arXiv:2104.00875
- Dual Discriminator Adversarial Distillation for Data-free Model Compression. Zhao, Haoran et al. TCSVT 2021
- See through Gradients: Image Batch Recovery via GradInversion. Yin, Hongxu et al. CVPR 2021
- Contrastive Model Inversion for Data-Free Knowledge Distillation. Fang, Gongfan et al. IJCAI 2021 [code]
- Graph-Free Knowledge Distillation for Graph Neural Networks. Deng, Xiang & Zhang, Zhongfei. arXiv:2105.07519

# Data-Free Quantization and Pruning
- Data-Free Network Quantization with Adversarial Knowledge Distillation. Choi, Yoojin et al. CVPRW 2020
- Zero-shot Adversarial Quantization. Liu, Yuang et al. CVPR 2021 [code]
- Towards Accurate Quantization and Pruning via Data-free Knowledge Transfer. arXiv:2010.07334
- Generative Low-bitwidth Data Free Quantization. Xu, Shoukai et al. ECCV 2020 [code]
- Learning in School: Multi-teacher Knowledge Inversion for Data-Free Quantization. Li, Yuhang et al. cvpr 2021

# Data-Free domain adaption
- Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation. ICML 2020
- Source-Free Domain Adaptation for Semantic Segmentation. Liu, Yuang et al. CVPR 2021
- Domain Impression: A Source Data Free Domain Adaptation Method. Kurmi et al. WACV 2021
- generalized source-free domain adaptation. ICCV 2021

# Data-Free model attack
- Zero-Shot Knowledge Distillation from a Decision-Based Black-Box Mode. Wang Zi. ICML 2021
- Delving into Data: Effectively Substitute Training for Black-box Attack. CVPR 2021
- MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation. Sanjay et al. arXiv:2005.03161


# Data-free Incremental Learning
- Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning | ICCV 2021
- 


# Data-Free Federated Learning
- Data-Free Knowledge Distillation for Heterogeneous Federated Learning. Zhu, Zhuangdi et al. ICML 2021




# Reference
https://github.com/FLHonker/Awesome-Knowledge-Distillation#data-free-kd
